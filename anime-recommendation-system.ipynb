{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecad4189-e11e-49d7-aedc-65185acdfe68",
   "metadata": {},
   "source": [
    "# ANIME RECOMMENDER SYSTEM\n",
    "\n",
    "This is an ***Anime Recommendation System*** that is implemented through various kinds of machine learning algorithms and includes different types of systems such as **Popularity Based (through Average Ratings)**, **Popularity Based (through Ranking/Score)**, **Content Based**, and **Collaborative-Filtering Based**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c840c5-4b73-4620-8675-388712fc0725",
   "metadata": {},
   "source": [
    "The ***'reviews.csv'*** file is no longer available in this project so some of the systems will not be able to function as they did before, including **Popularity Based (through Average Ratings) System** and **Collaborative-Filtering Based System**. However, the code for these recommender systems would remain effective had the ***'reviews.csv'*** file been present, so they have been left in this notebook for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9259aaaa-a26c-4974-ad68-4efb84a6dfc9",
   "metadata": {},
   "source": [
    "The dataset that was available for the anime i.e. ***'animes.csv'*** has scarce and scattered information regarding user ratings as not many users have rated more than one kind of anime so there's not much co-relation between users based on the kind of anime they have rated on, thus, making **Collaborative Filtering** ineffective for this dataset. \n",
    "\n",
    "However, the algorithm would remain more or less the same, so, when working with a different dataset, the **Collaborative-Filtering Based System** can be implemented, and subsequently, a **Hybrid Recommender System** can be implemented as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b0277-498a-4e87-93df-524adf633442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3496b-1c69-4668-8992-154f0756786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes = pd.read_csv('animes.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2de899be-21c9-46b6-81b4-62a002693d99",
   "metadata": {},
   "source": [
    "reviews = pd.read_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e14ac-d196-4f6e-829b-68280f7800fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes.head(1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4387a8ae-600d-4c8d-8a07-15542347dd21",
   "metadata": {},
   "source": [
    "reviews.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efda98e5-69dc-4254-8321-1f278355b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(animes.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa71d3fb-3654-4e6a-942f-0665b0bf158d",
   "metadata": {},
   "source": [
    "print(reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee27bb-5652-490a-bed6-d9af879e8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes.isnull().sum() # to check for any null values"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70e23151-f1ca-4339-9270-5f42b5c49fb0",
   "metadata": {},
   "source": [
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a4b0f7-af56-45f5-8c21-6550192e318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes.duplicated().sum() # to check duplicates"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afbfff24-da06-40ff-afdf-21939c917d4e",
   "metadata": {},
   "source": [
    "reviews.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf683c-e88c-47c0-9f8a-e52f4dc82965",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes.drop_duplicates(inplace = True)\n",
    "animes.duplicated().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c22820f-fd4f-4f49-86e3-14601c1bcfe9",
   "metadata": {},
   "source": [
    "reviews.drop_duplicates(inplace = True)\n",
    "reviews.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3364728-423f-45b1-a8ed-96804657cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT THE RELEASE DATE FROM THE AIRED COLUMN IN ANIMES CSV\n",
    "\n",
    "# extracting aired column from animes containing release date and final date\n",
    "\n",
    "release_date = animes['aired'].values\n",
    "release_date_list = [] \n",
    "\n",
    "# extracting aired column values and seperating the release date from final date\n",
    "# and storing the release date part\n",
    "\n",
    "for i in range(len(release_date)):\n",
    "    release_date_list.append(release_date[i].split(\"to\")[0])\n",
    "    \n",
    "animes['aired'] = release_date_list\n",
    "animes.rename(columns = {'aired':'release_date'}, inplace = True)\n",
    "animes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320f7b44-5ddd-466e-bda1-1f95d04da49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping NULL values from the animes dataset\n",
    "\n",
    "animes.dropna(axis = 0, how = 'any', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85267fe-c29f-499a-864e-0aa5ff381c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1053876-19bf-449f-b69e-de9fbc86cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c171a1b5-0462-4d41-b002-b15a02be69e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the uid aka anime id in animes.csv to anime_uid so we can merge\n",
    "# animes and reviews\n",
    "\n",
    "animes.rename(columns = {'uid':'anime_uid'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ac154-d057-4810-a2ca-639516d18c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes = animes[['anime_uid', 'title', 'synopsis', 'genre', 'release_date', 'episodes', 'members', \n",
    "                 'popularity', 'ranked', 'score', 'img_url']]\n",
    "animes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8972d232-a8f6-4364-86fa-ac0a3af74e3f",
   "metadata": {},
   "source": [
    "reviews = reviews[['uid', 'anime_uid', 'score']]\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a15f9a-e624-4f4e-a93f-5d9ff33aafa9",
   "metadata": {},
   "source": [
    "## POPULARITY BASED RECOMMENDATION SYSTEM (USING AVERAGE RATINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1de11d-ceaa-4759-8e9c-aa3f6277dd20",
   "metadata": {},
   "source": [
    "This type of recommendation system needs the ***'reviews.csv'*** file that is no longer available in this project. However, the code would remain applicable if the ***'reviews.csv'*** file had been present."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7a07274-2408-4b32-a68a-94e21f8d2a8c",
   "metadata": {},
   "source": [
    "anime_w_ratings = animes.merge(reviews, on = 'anime_uid')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9da2341-2db1-41bf-a132-1ac3ac97791f",
   "metadata": {},
   "source": [
    "num_rating_df = animes.groupby('title').count()['score_y'].reset_index()\n",
    "num_rating_df.rename(columns = {'score_y':'num_ratings'}, inplace = True)\n",
    "num_rating_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c518d6b-747f-4f14-ac51-739abb5fa6a6",
   "metadata": {},
   "source": [
    "avg_rating_df = anime_w_ratings.groupby('title').mean()['score_y'].reset_index()\n",
    "avg_rating_df.rename(columns = {'score_y':'avg_ratings'}, inplace = True)\n",
    "avg_rating_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01d09cd0-e040-4265-b826-d9e1885d126a",
   "metadata": {},
   "source": [
    "popular_df_w_avg = num_rating_df.merge(avg_rating_df, on = 'title')\n",
    "popular_df_w_avg"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eea0c650-ac0c-49f4-ace7-b996d55a9aa1",
   "metadata": {},
   "source": [
    "# keep the animes whose num rating is greater than 250\n",
    "\n",
    "popular_df_w_avg = popular_df_w_avg[popular_df_w_avg['num_ratings'] >= 600].sort_values('avg_ratings', ascending = False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b54998de-73e2-4bb7-b959-939b9813af66",
   "metadata": {},
   "source": [
    "popular_df_w_avg = popular_df_w_avg.merge(animes, on = 'title')[['anime_uid', 'title', 'synopsis', 'genre', 'release_date', 'episodes', 'score', 'img_url']]\n",
    "popular_df_w_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7003cb6-f67d-42ea-82be-b02a6dfdaa6e",
   "metadata": {},
   "source": [
    "## POPULARITY BASED RECOMMENDER SYSTEM (USING THE SCORE COLUMN IN THE ANIMES.CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e14aad-c905-4e82-9581-b09360bb695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_df = animes[animes['members'] >= 1000000].sort_values('popularity', ascending = True)\n",
    "popular_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b656b18-d11f-46ac-97da-994369fad4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_df = popular_df[['anime_uid', 'title', 'synopsis', 'genre', 'release_date',\n",
    "           'episodes', 'score', 'img_url']][0:10]\n",
    "popular_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b078f-1e80-44f5-a9b1-2598540ecd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_details = animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655dd69d-7e6f-41f0-8ed1-08128ada29c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(anime_details, open('anime_details.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb971a-8d26-41bb-b782-340f951ecb68",
   "metadata": {},
   "source": [
    "## CONTENT BASED RECOMMENDATION SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3163d708-0259-4e78-8688-0d560c363de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Following their participation at the Inter-High, the Karasuno High School volleyball team attempts to refocus their efforts, aiming to conquer the Spring tournament instead.  \\r\\n \\r\\nWhen they receive an invitation from long-standing rival Nekoma High, Karasuno agrees to take part in a large training camp alongside many notable volleyball teams in Tokyo and even some national level players. By playing with some of the toughest teams in Japan, they hope not only to sharpen their skills, but also come up with new attacks that would strengthen them. Moreover, Hinata and Kageyama attempt to devise a more powerful weapon, one that could possibly break the sturdiest of blocks.  \\r\\n \\r\\nFacing what may be their last chance at victory before the senior players graduate, the members of Karasuno's volleyball team must learn to settle their differences and train harder than ever if they hope to overcome formidable opponents old and new—including their archrival Aoba Jousai and its world-class setter Tooru Oikawa. \\r\\n \\r\\n[Written by MAL Rewrite]\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This fetches the synopsis of the first anime 'Haikyuu' in this case\n",
    "\n",
    "animes_cbrs = animes\n",
    "\n",
    "animes_cbrs['synopsis'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817dd9a1-32c9-4bc5-929c-60b96000c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_cbrs['synopsis'] = animes_cbrs['synopsis'].apply(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20b381-bdbb-46bd-be91-956bdd05c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_cbrs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba7b9a9-3ed8-40d6-a8dd-3e5b5185b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28742282-decb-441c-8465-f3c11ee08118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(obj):\n",
    "    L = []\n",
    "    for i in ast.literal_eval(obj):\n",
    "        L.append(i)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ac275-c46b-4589-9007-06e72a77934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_cbrs['genre'] = animes_cbrs['genre'].apply(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd56234-4ec1-4a78-ad8a-97dc275e6b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_cbrs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066ee1b-b06e-40db-8007-078f97b75425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any spaces between the genres, say Slice of Life to SliceofLife to avoid \n",
    "# errors in the recommendation system\n",
    "\n",
    "animes_cbrs['genre'].apply(lambda x:[i.replace(\" \", \"\") for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c746e-b4c9-473e-8cf9-605a2bef79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_cbrs['tags'] = animes_cbrs['synopsis'] + animes_cbrs['genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7664f2d-9972-437d-95d5-3795645fca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_cbrs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b05119-e183-433f-9c15-cb9cde961daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the list of tags to a string\n",
    "\n",
    "animes_cbrs['tags'] = animes_cbrs['tags'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad0f1c-376f-49f4-91f4-99601f1ecf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the tags in the same string \n",
    "\n",
    "animes_cbrs['tags'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0614650-70da-4c11-8cb3-e98a09f471c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the string into lowercase\n",
    "\n",
    "animes_cbrs['tags'] = animes_cbrs['tags'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f150450-5fd2-439b-bd9d-d6d0ed5f6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_cbrs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927018c6-90f9-47fc-80fe-513f99c43ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_cbrs['tags'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bab509a-107d-415d-8bc1-95a0df2e036a",
   "metadata": {},
   "source": [
    "### STEMMING\n",
    "\n",
    "We will apply stemming on the list of words as there are multiple variations of the same word.\n",
    "\n",
    "nltk is a famous natural language processing library. Install nltk using **'pip install nltk'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf18e2ef-be52-483a-9e6e-35ec35d36eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem(text):\n",
    "    y = []\n",
    "    for i in text.split():\n",
    "        y.append(ps.stem(i))\n",
    "    \n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9f2e6-7e7e-4672-a7aa-9254e3e5de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_cbrs['tags'] = animes_cbrs['tags'].apply(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f4de4-68c9-42be-9019-f8fee7e8c0a0",
   "metadata": {},
   "source": [
    "## TEXT VECTORIZATION USING BAG OF WORDS TECHNIQUE\n",
    "\n",
    "Combine all the words (tags in this case) that are there like word1 + word2 + ... + wordn into a one large string.\n",
    "\n",
    "In that huge string, calculate which words have the highest frequency (after calculating the frequency of each word) and extract, say, 5000 words that have the highest frequency.\n",
    "\n",
    "Then we check with each tag of the anime and check how many times each of those 5000 words are present there or not (which will be 0 if the word is not present in that particular anime's tag).\n",
    "\n",
    "This will look somewhat like this:\n",
    "\n",
    "<pre><code>\n",
    "            Word1     Word2     Word3    .....     Word5000\n",
    " Anime1       5         3         0                   1                             \n",
    " Anime2       2         0         1                   3\n",
    " Anime3       1         1         1                   1\n",
    " .\n",
    " .\n",
    " .\n",
    " Anime5000    5         2         5                   5\n",
    "</code></pre>\n",
    "\n",
    "*(with shape (5000, 5000))*\n",
    "\n",
    "That one row of 5, 3, 0, ... 1 is actually a vector. The anime has been converted into a vector in a 5000 dimensional space.\n",
    "\n",
    "Now, when someone says they like a particular anime, we will fetch the 5 closest vectors of that anime.\n",
    "\n",
    "Therefore, we converted our entire text into a vector, so now, every anime is a vector in a 5000 dimensional space\n",
    "\n",
    "We have taken 5000 words when we could have taken more. However, we need the most efficient/best performance in the least amount of words as when the number of words increases, so does the dimensionality of the data, which is problematic, so it's better to take the least amount of words possible.\n",
    "\n",
    "We will not be considering stop words (aka words that are used for sentence formation but add no value/contribution to the actual meaning of the sentence, like are, and, or, to, from, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec18748-b703-4bc8-be5f-1b991a2e7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 5000, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f791610-d96a-4f33-860f-da3403aab2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there will be many 0 values in this. by default, CountVectorizer returns a SciPy sparse matrix\n",
    "# so we will convert it to a numpy array as we need it\n",
    "\n",
    "vectors = cv.fit_transform(animes_cbrs['tags']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc33a3e3-3292-423a-9a95-5550710cd2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors[0] # this matrix will be sparse as hardly 5000 words will be there in a single anime"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51bd2f27-3879-466c-87da-c3de07a80421",
   "metadata": {},
   "source": [
    "You can fetch the 5000 most frequent words in the anime (arranged in alphabetic order) through:\n",
    "\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7374fb-8861-40d9-a59e-ee69b301e86c",
   "metadata": {},
   "source": [
    "### We will be calculating the Cosine Similarity of one vector with all the other vectors and repeat it for all the vectors (each anime with each anime)\n",
    "\n",
    "While dealing with higher dimensional data is, euclidean distance should be avoided as it fails the higher the dimensional data is. It is not a reliable source of measure to calculate the distance when dealing with data of higher dimensionality so we will be using the cosine distance between the vectors i.e. the (θ) angle instead. \n",
    "\n",
    "The smaller the angle is, the lesser the distance, therefore, the two vectors (anime) will be more similar. Cosine distance is inversely proportional to cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc1575-fe05-452e-bbf7-894949b6fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb3e12-b642-4fba-931b-f46dc192d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a632a42a-e9cd-4936-a940-59ec5975d4ce",
   "metadata": {},
   "source": [
    "This method calculates the similarity between each vector, with the anime having the most similarity (1) with itself. Hence, there will be a diagonal representation where every anime will have the most similarity with itself, which will be represented through 1 (the cosine similarity is from 0 to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abb2a20-33fc-43bb-a8a4-8afbfe421c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity[0] # this is the similarity score of the first anime with every anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6abc5cc-9157-4bb2-a97a-12c77f771cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity[1] # this is the similarity score of the second anime with every anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6519bb9-4b4e-4caa-96c0-fd84e6c66cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(anime):\n",
    "    \n",
    "    index = animes_cbrs[animes_cbrs['title'] == anime].index[0]\n",
    "    distances = sorted(list(enumerate(similarity[index])), reverse = True, key = lambda x: x[1])\n",
    "    \n",
    "    data = []\n",
    "    for i in distances[1:6]:\n",
    "        item = []\n",
    "        #print(animes_cbrs.iloc[i[0]].title)\n",
    "        temp_df = animes_cbrs[animes_cbrs['title'] == animes_cbrs.iloc[i[0]].title]\n",
    "        item.extend(list(temp_df['title'].values))\n",
    "        \n",
    "        item.extend(list(anime_details[anime_details['title'].isin(temp_df['title'])]['synopsis'].values))\n",
    "        \n",
    "        item.extend(list(temp_df['genre'].values))\n",
    "        item.extend(list(temp_df['release_date'].values))\n",
    "        item.extend(list(temp_df['episodes'].values))\n",
    "        item.extend(list(temp_df['score'].values))\n",
    "        item.extend(list(temp_df['img_url'].values))\n",
    "        \n",
    "        data.append(item)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dab00e-bf96-452c-9fd5-4331f5510e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend('Death Note')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6765da0c-8964-4d14-8d9c-2a45da8a9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(animes_cbrs, open('animes_cbrs.pkl', 'wb'))\n",
    "pickle.dump(similarity, open('similarity.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652069d-c27d-4da8-833e-fb63aa9aa067",
   "metadata": {},
   "source": [
    "## COLLABORATIVE BASED RECOMMENDATION SYSTEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f26c63-c061-44d3-b506-264ae1ff9fff",
   "metadata": {},
   "source": [
    "We merge the ***'animes.csv'*** and the ***'reviews.csv'*** to have a dataframe that will be somewhat as follows:\n",
    "\n",
    "<pre><code>\n",
    "           User 1     User 2    .....     User n\n",
    " Anime 1      5           3                  10\n",
    " Anime 2      1           0                  3\n",
    " Anime 3      5           6                  7\n",
    " .\n",
    " .\n",
    " .\n",
    " Anime n      3           9                  6\n",
    "</code></pre>\n",
    "\n",
    "To make the recommender system more accurate, it is better to have a certain threshold for the number of ratings done by a user and consider only those users that pass that threshold, as they will be reliable users and will make the system more efficient and accurate.\n",
    "\n",
    "You can also consider only those animes that has been rated by atleast ***'N'*** users so as to not recommend anime that are really not well known and may not be the most accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e55632a-d565-4f92-ab55-288d8961b5aa",
   "metadata": {},
   "source": [
    "We will be using the merged dataset **(animes.csv + reviews.csv)** aka **anime_w_ratings**\n",
    "\n",
    "We will check every user and how many animes they have rated, then return ***True*** for the ones that have rated more than ***'N'*** animes, and **False** for those who haven't. We will temporarily store this in a variable x.\n",
    "\n",
    "<pre><code>\n",
    "counts = animes_cfrs['uid'].value_counts()\n",
    "animes_cfrs = animes_cfrs[animes_cfrs['uid'].isin(counts[counts >= 50].index)]\n",
    "</code></pre>\n",
    "\n",
    "<pre><code>\n",
    "x = anime_w_ratings.groupby('uid').count()['score_y'] >= 50\n",
    "valid_users = x[x].index\n",
    "</code></pre>\n",
    "\n",
    "These will be all the user-ids have rated atleast 50 anime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992344c-98ab-408b-8245-4e63d0427de5",
   "metadata": {},
   "source": [
    "Fetch only those rows from the dataframe in which the users are valid users:\n",
    "<pre><code>\n",
    "filtered_rating = anime_w_ratings[anime_w_ratings['uid'].isin(valid_users)]\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0aa284-6bca-4674-b029-980eddad33c4",
   "metadata": {},
   "source": [
    "Fetch all the anime that have ratings more than or equal to 50.\n",
    "\n",
    "<pre><code>\n",
    "y = filtered_rating.groupby('title').count()['score_y'] >= 50\n",
    "valid_animes = y[y].index \n",
    "</code></pre>\n",
    "\n",
    "These are all the animes that have more than or equal to 50 ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e62a35-4c5a-4a4a-8ab8-ef77b31ed99c",
   "metadata": {},
   "source": [
    "Fetch only those rows from the dataframe in which the anime that are valid animes:\n",
    "\n",
    "<pre><code>\n",
    "animes_cfrs = filtered_rating[filtered_rating['title'].isin(valid_animes)]\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0fdf99-6468-4163-8227-5796271e8619",
   "metadata": {},
   "source": [
    "Create a spreadsheet-style pivot table as a DataFrame which will store the anime titles along with their user-id and the scores given by the users.\n",
    "\n",
    "<pre><code>\n",
    "pt = animes_cfrs.pivot_table(index = 'title', columns = 'uid', values = 'score_y')\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97f55f-fb59-486f-83f1-e87cbf727462",
   "metadata": {},
   "source": [
    "Replace all the NULL values with 0 instead.\n",
    "\n",
    "<pre><code>\n",
    "pt.fillna(0, inplace = True)\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adafb62c-a470-4158-b577-c406ce59f7e6",
   "metadata": {},
   "source": [
    "Compute the distance of each anime with all the other anime\n",
    "\n",
    "<pre><code>\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_scores = cosine_similarity(pt)\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f1444-4928-43e3-a591-2171aaef9736",
   "metadata": {},
   "source": [
    "RECOMMENDER FUNCTION:\n",
    "\n",
    "<pre><code>\n",
    "def recommend_cfrs(anime_name):\n",
    "    index fetch\n",
    "    index = np.where(pt.index == anime_name)[0][0]\n",
    "    distances = similarity_scores[index]\n",
    "    similar_items = sorted(list(enumerate(similarity_scores[index])), key = lambda x:x[1], reverse = True)[1:6]\n",
    "\n",
    "    for i in similar_items:\n",
    "        print(pt.index[i[0]])\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57551be7-8f39-49c5-903f-e9621f8231ca",
   "metadata": {},
   "source": [
    "Check whether the recommender system is working by calling the function and giving the name of the anime you want recommendations for as the parameter:\n",
    "\n",
    "<pre><code>\n",
    "recommend_cfrs('Death Note')\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289dbe21-3129-4ffa-b75c-4116e1c59dc9",
   "metadata": {},
   "source": [
    "It seems like there aren't any users who have voted on more than one kind of anime, so, since the data we are working with is very sparse and scattered, it would seem that the collaborative filtering method is not working too well as a recommendation system for this dataset (while the methodology is the same so it will work for any other dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32845313-3a23-4c4e-8926-fae24c85cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e84ca8-bbaf-412d-b634-1e0f9ee9bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(popular_df, open('popular.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
